{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Regressive feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lags(\n",
    "    df,\n",
    "    lags,\n",
    "    column,\n",
    "    ts_id=None,\n",
    "    use_32_bit=False\n",
    "):\n",
    "    added_features = []\n",
    "    for l in lags:\n",
    "        lag_name = f\"{column}_lag_{l}\"\n",
    "        if ts_id:\n",
    "            df[lag_name] = df.groupby(ts_id)[column].shift(l)\n",
    "        else:\n",
    "            df[lag_name] = df[column].shift(l)\n",
    "\n",
    "        if use_32_bit:\n",
    "            df[lag_name] = df[lag_name].astype('float32')\n",
    "\n",
    "        added_features.append(lag_name)\n",
    "\n",
    "    return df, added_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(\n",
    "    df,\n",
    "    rolls,\n",
    "    column,\n",
    "    agg_funcs=[\"mean\", \"std\"],\n",
    "    ts_id=None,\n",
    "    n_shift=1,\n",
    "    use_32_bit=False\n",
    "):\n",
    "    added_features = []\n",
    "    for l in rolls:\n",
    "        for agg in agg_funcs:\n",
    "            feature_name = f\"{column}_rolling_{l}_{agg}\"\n",
    "            if ts_id:\n",
    "                df[feature_name] = df.groupby(ts_id)[column].shift(n_shift).rolling(l).agg(agg)\n",
    "            else:\n",
    "                df[feature_name] = df[column].shift(n_shift).rolling(l).agg(agg)\n",
    "            if use_32_bit:\n",
    "                df[feature_name] = df[feature_name].astype('float32')\n",
    "            added_features.append(feature_name)\n",
    "\n",
    "    return df, added_features\n",
    "\n",
    "def add_seasonal_rolling_features(\n",
    "    df,\n",
    "    seasonal_periods,\n",
    "    rolls,\n",
    "    column,\n",
    "    agg_funcs=[\"mean\", \"std\"],\n",
    "    ts_id=None,\n",
    "    n_shift=1,\n",
    "    use_32_bit=False\n",
    "):\n",
    "    added_features = []\n",
    "    for sp in seasonal_periods:\n",
    "        for l in rolls:\n",
    "            for agg in agg_funcs:\n",
    "                if ts_id:\n",
    "                    grouped = df.groupby(ts_id)[column].shift(n_shift * sp)\n",
    "                else:\n",
    "                    grouped = df[column].shift(n_shift * sp)\n",
    "\n",
    "                rolling_name = f\"{column}_{sp}_seasonal_rolling_{l}_{agg}\"\n",
    "                df[rolling_name] = grouped.rolling(l).agg(agg)\n",
    "                if use_32_bit:\n",
    "                    df[rolling_name] = df[rolling_name].astype('float32')\n",
    "                added_features.append(rolling_name)\n",
    "\n",
    "    return df, added_features\n",
    "\n",
    "def add_ewma(\n",
    "    df,\n",
    "    column,\n",
    "    alphas=[0.5],\n",
    "    spans=None,\n",
    "    ts_id=None,\n",
    "    n_shift=1,\n",
    "    use_32_bit=False\n",
    "):\n",
    "    added_features = []\n",
    "    if spans is None:\n",
    "        spans = [None] * len(alphas)\n",
    "\n",
    "    for alpha, span in zip(alphas, spans):\n",
    "        if ts_id:\n",
    "            grouped = df.groupby(ts_id)[column].shift(n_shift)\n",
    "        else:\n",
    "            grouped = df[column].shift(n_shift)\n",
    "\n",
    "        ewma_name = f\"{column}_ewma_{'span' if span is not None else 'alpha'}_{span or alpha}\"\n",
    "        df[ewma_name] = grouped.ewm(alpha=alpha, span=span, adjust=False).mean()\n",
    "        if use_32_bit:\n",
    "            df[ewma_name] = df[ewma_name].astype('float32')\n",
    "        added_features.append(ewma_name)\n",
    "\n",
    "    return df, added_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## temporal feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "def time_features_from_frequency_str(freq_str):\n",
    "    features_by_offsets = {\n",
    "        'A': [],\n",
    "        'Q': [\"Month\", \"Quarter\", \"Is_quarter_end\", \"Is_quarter_start\", \"Is_year_end\", \"Is_year_start\"],\n",
    "        'M': [\"Month\", \"Quarter\", \"Is_quarter_end\", \"Is_quarter_start\", \"Is_year_end\", \"Is_year_start\"],\n",
    "        'W': [\"Month\", \"Quarter\", \"Is_quarter_end\", \"Is_quarter_start\", \"Is_year_end\", \"Is_year_start\", \"Is_month_start\", \"Week\"],\n",
    "        'D': [\"Month\", \"Quarter\", \"Is_quarter_end\", \"Is_quarter_start\", \"Is_year_end\", \"Is_year_start\", \"Is_month_start\", \"Week\", \"Day\", \"Dayofweek\", \"Dayofyear\"],\n",
    "        'B': [\"Month\", \"Quarter\", \"Is_quarter_end\", \"Is_quarter_start\", \"Is_year_end\", \"Is_year_start\", \"Is_month_start\", \"Week\", \"Day\", \"Dayofweek\", \"Dayofyear\"],\n",
    "        'H': [\"Month\", \"Quarter\", \"Is_quarter_end\", \"Is_quarter_start\", \"Is_year_end\", \"Is_year_start\", \"Is_month_start\", \"Week\", \"Day\", \"Dayofweek\", \"Dayofyear\", \"Hour\"],\n",
    "        'T': [\"Month\", \"Quarter\", \"Is_quarter_end\", \"Is_quarter_start\", \"Is_year_end\", \"Is_year_start\", \"Is_month_start\", \"Week\", \"Day\", \"Dayofweek\", \"Dayofyear\", \"Hour\", \"Minute\"]\n",
    "    }\n",
    "    return features_by_offsets.get(to_offset(freq_str).__class__, [])\n",
    "\n",
    "def add_lags(df, lags, column, ts_id=None, use_32_bit=False):\n",
    "    for l in lags:\n",
    "        lag_name = f\"{column}_lag_{l}\"\n",
    "        df[lag_name] = df.groupby(ts_id)[column].shift(l) if ts_id else df[column].shift(l)\n",
    "        if use_32_bit:\n",
    "            df[lag_name] = df[lag_name].astype('float32')\n",
    "    return df\n",
    "\n",
    "def add_temporal_features(df, field_name, frequency, add_elapsed=True, prefix=None, drop=True, use_32_bit=False):\n",
    "    prefix = prefix or re.sub(\"[Dd]ate$\", \"\", field_name) + \"_\"\n",
    "    for n in time_features_from_frequency_str(frequency):\n",
    "        df[prefix + n] = getattr(df[field_name].dt, n.lower()).astype('float32' if use_32_bit else 'float64')\n",
    "    if add_elapsed:\n",
    "        df[prefix + \"Elapsed\"] = df[field_name].astype('int64') // 10**9\n",
    "        df[prefix + \"Elapsed\"] = df[prefix + \"Elapsed\"].astype('float32' if use_32_bit else 'int64')\n",
    "    if drop:\n",
    "        df.drop(field_name, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def add_fourier_features(df, column_to_encode, max_value=None, n_fourier_terms=1, use_32_bit=False):\n",
    "    max_value = max_value or df[column_to_encode].max()\n",
    "    seasonal_cycle = df[column_to_encode].astype(int)\n",
    "    for i in range(1, n_fourier_terms + 1):\n",
    "        df[f\"{column_to_encode}_sin_{i}\"] = np.sin((2 * np.pi * seasonal_cycle * i) / max_value)\n",
    "        df[f\"{column_to_encode}_cos_{i}\"] = np.cos((2 * np.pi * seasonal_cycle * i) / max_value)\n",
    "        if use_32_bit:\n",
    "            df[f\"{column_to_encode}_sin_{i}\"] = df[f\"{column_to_encode}_sin_{i}\"].astype('float32')\n",
    "            df[f\"{column_to_encode}_cos_{i}\"] = df[f\"{column_to_encode}_cos_{i}\"].astype('float32')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_list(list1, list2):\n",
    "    return list(set(list1).intersection(set(list2)))\n",
    "\n",
    "def difference_list(list1, list2):\n",
    "    return list(set(list1)- set(list2))\n",
    "\n",
    "def union_list(list1, list2):\n",
    "    return list(set(list1).union(set(list2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "def intersect_list(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def difference_list(a, b):\n",
    "    return list(set(a) - set(b))\n",
    "\n",
    "class MissingValueConfig:\n",
    "    bfill_columns: List = field(default_factory=list)\n",
    "    ffill_columns: List = field(default_factory=list)\n",
    "    zero_fill_columns: List = field(default_factory=list)\n",
    "\n",
    "    def impute_missing_values(self, df: pd.DataFrame):\n",
    "        df = df.copy()\n",
    "        bfill_columns = intersect_list(df.columns, self.bfill_columns)\n",
    "        df[bfill_columns] = df[bfill_columns].fillna(method=\"bfill\")\n",
    "        ffill_columns = intersect_list(df.columns, self.ffill_columns)\n",
    "        df[ffill_columns] = df[ffill_columns].fillna(method=\"ffill\")\n",
    "        zero_fill_columns = intersect_list(df.columns, self.zero_fill_columns)\n",
    "        df[zero_fill_columns] = df[zero_fill_columns].fillna(0)\n",
    "        check = df.isnull().any()\n",
    "        missing_cols = check[check].index.tolist()\n",
    "        missing_numeric_cols = intersect_list(\n",
    "            missing_cols, df.select_dtypes([np.number]).columns.tolist()\n",
    "        )\n",
    "        missing_object_cols = intersect_list(\n",
    "            missing_cols, df.select_dtypes([\"object\"]).columns.tolist()\n",
    "        )\n",
    "        df[missing_numeric_cols] = df[missing_numeric_cols].fillna(\n",
    "            df[missing_numeric_cols].mean()\n",
    "        )\n",
    "        df[missing_object_cols] = df[missing_object_cols].fillna(\"NA\")\n",
    "        return df\n",
    "\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    date: List = field(default_factory=list)\n",
    "    target: str = field(default=None)\n",
    "    original_target: str = field(default=None)\n",
    "    continuous_features: List[str] = field(default_factory=list)\n",
    "    categorical_features: List[str] = field(default_factory=list)\n",
    "    boolean_features: List[str] = field(default_factory=list)\n",
    "    index_cols: str = field(default_factory=list)\n",
    "    exogenous_features: List[str] = field(default_factory=list)\n",
    "    feature_list: List[str] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.feature_list = (\n",
    "            self.categorical_features + self.continuous_features + self.boolean_features\n",
    "        )\n",
    "        if self.original_target is None:\n",
    "            self.original_target = self.target\n",
    "\n",
    "    def get_X_y(self, df: pd.DataFrame, categorical: bool = False, exogenous: bool = False):\n",
    "        feature_list = self.continuous_features\n",
    "        if categorical:\n",
    "            feature_list += self.categorical_features + self.boolean_features\n",
    "        if not exogenous:\n",
    "            feature_list = list(set(feature_list) - set(self.exogenous_features))\n",
    "        feature_list = list(set(feature_list))\n",
    "        delete_index_cols = list(set(self.index_cols) - set(self.feature_list))\n",
    "        X, y, y_orig = (\n",
    "            df.loc[:, set(feature_list + self.index_cols)]\n",
    "            .set_index(self.index_cols, drop=False)\n",
    "            .drop(columns=delete_index_cols),\n",
    "            df.loc[:, [self.target] + self.index_cols].set_index(\n",
    "                self.index_cols, drop=True\n",
    "            )\n",
    "            if self.target in df.columns\n",
    "            else None,\n",
    "            df.loc[:, [self.original_target] + self.index_cols].set_index(\n",
    "                self.index_cols, drop=True\n",
    "            )\n",
    "            if self.original_target in df.columns\n",
    "            else None,\n",
    "        )\n",
    "        return X, y, y_orig\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model: BaseEstimator = field(default=None)\n",
    "    name: str = field(default=None)\n",
    "    normalize: bool = field(default=False)\n",
    "    fill_missing: bool = field(default=True)\n",
    "    encode_categorical: bool = field(default=False)\n",
    "    categorical_encoder: BaseEstimator = field(default=None)\n",
    "\n",
    "    def clone(self):\n",
    "        self.model = clone(self.model)\n",
    "        return self\n",
    "\n",
    "class MLForecast:\n",
    "    def __init__(self, model_config: ModelConfig, feature_config: FeatureConfig, missing_config: MissingValueConfig = None, target_transformer: object = None):\n",
    "        self.model_config = model_config\n",
    "        self.feature_config = feature_config\n",
    "        self.missing_config = missing_config\n",
    "        self.target_transformer = target_transformer\n",
    "        self._model = clone(model_config.model)\n",
    "        if self.model_config.normalize:\n",
    "            self._scaler = StandardScaler()\n",
    "        if self.model_config.encode_categorical:\n",
    "            self._cat_encoder = self.model_config.categorical_encoder\n",
    "            self._encoded_categorical_features = self.feature_config.categorical_features\n",
    "\n",
    "    def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "\n",
    "def calculate_diversity(ens, diversity_matrix, default_div=1):\n",
    "    if len(ens) == 1:\n",
    "        return default_div\n",
    "    return np.mean([diversity_matrix.loc[i, j] for i, j in itertools.combinations(ens, 2)])\n",
    "\n",
    "def calculate_performance(ens, pred_wide, target, ensemble_func=np.mean, metric_func=None):\n",
    "    pred = ensemble_func(pred_wide[ens], axis=1)\n",
    "    act = pred_wide[target]\n",
    "    return metric_func(pred, act)\n",
    "\n",
    "def generate_random_candidate(candidates):\n",
    "    return random.sample(candidates, 1)\n",
    "\n",
    "def generate_best_candidate(objective, solution, candidates):\n",
    "    cost = [objective(solution + [c]) for c in candidates]\n",
    "    return [candidates[np.argmin(cost)]], np.min(cost)\n",
    "\n",
    "def _initialize(candidates, objective, init):\n",
    "    if init == \"best\":\n",
    "        cost = [objective([c]) for c in candidates]\n",
    "        return [candidates[np.argmin(cost)]], np.min(cost)\n",
    "    elif init == \"random\":\n",
    "        c = generate_random_candidate(candidates)\n",
    "        return c, objective(c)\n",
    "\n",
    "def greedy_optimization(objective, candidates, verbose=True):\n",
    "    solution, solution_eval = _initialize(candidates, objective, init=\"best\")\n",
    "    candidates.remove(solution[0])\n",
    "    while candidates:\n",
    "        _candidate, candidate_eval = generate_best_candidate(objective, solution, candidates)\n",
    "        candidate = solution + _candidate\n",
    "        if candidate_eval <= solution_eval:\n",
    "            solution, solution_eval = candidate, candidate_eval\n",
    "            candidates.remove(_candidate[0])\n",
    "    return solution, solution_eval\n",
    "\n",
    "def stochastic_hillclimbing(objective, candidates, n_iterations=None, init=\"best\", verbose=True, random_state=42):\n",
    "    random.seed(random_state)\n",
    "    n_iterations = len(candidates) * 2 if n_iterations is None else n_iterations\n",
    "    solution, solution_eval = _initialize(candidates, objective, init)\n",
    "    candidates.remove(solution[0])\n",
    "    for i in range(n_iterations):\n",
    "        _candidate = generate_random_candidate(candidates)\n",
    "        candidate = solution + _candidate\n",
    "        candidate_eval = objective(candidate)\n",
    "        if candidate_eval <= solution_eval:\n",
    "            solution, solution_eval = candidate, candidate_eval\n",
    "            candidates.remove(_candidate[0])\n",
    "\n",
    "    return solution, solution_eval\n",
    "\n",
    "def _decay_temperature(current_temp, alpha, kind=\"linear\"):\n",
    "    return current_temp - alpha if kind == \"linear\" else current_temp / alpha\n",
    "\n",
    "def initialize_temperature_range(objective, candidate_pool, p_range, n_iterations=100):\n",
    "    diff_l = []\n",
    "    candidates = generate_random_candidate(candidate_pool)\n",
    "    candidate_score = objective(candidates)\n",
    "    for _ in range(n_iterations):\n",
    "        cand = generate_random_candidate(candidate_pool)\n",
    "        candidates += cand\n",
    "        candidate_pool.remove(cand[0])\n",
    "        diff = candidate_score - objective(candidates)\n",
    "        diff_l.append(diff)\n",
    "    avg_diff = np.median(np.abs(diff_l))\n",
    "    return (-avg_diff / math.log(p_range[0]), -avg_diff / math.log(p_range[1]))\n",
    "\n",
    "def simulated_annealing(objective, candidates, n_iterations, p_range=(0.7, 0.001), t_range=None, init=\"best\", temperature_decay=\"linear\", verbose=True, random_state=42):\n",
    "    random.seed(random_state)\n",
    "    n_iterations = min(n_iterations, int(len(candidates) * 1.2))\n",
    "    if t_range is None:\n",
    "        t_range = initialize_temperature_range(objective, candidates, p_range)\n",
    "    alpha = (t_range[0] - t_range[1]) / (n_iterations - 1) if temperature_decay == \"linear\" else math.pow((t_range[0] / t_range[1]), 1 / (n_iterations - 1))\n",
    "    best_solution, best_solution_eval = _initialize(candidates, objective, init)\n",
    "    candidates.remove(best_solution[0])\n",
    "    current_temp = t_range[0]\n",
    "    for i in range(n_iterations):\n",
    "        _candidate = generate_random_candidate(candidates)\n",
    "        candidate = best_solution + _candidate\n",
    "        candidate_eval = objective(candidate)\n",
    "        diff = best_solution_eval - candidate_eval\n",
    "        if diff > 0 or random.uniform(0, 1) < math.exp(-abs(diff) / current_temp):\n",
    "            best_solution, best_solution_eval = candidate, candidate_eval\n",
    "            candidates.remove(_candidate[0])\n",
    "        current_temp = _decay_temperature(current_temp, alpha, temperature_decay)\n",
    "        if not candidates:\n",
    "            break\n",
    "    return best_solution, best_solution_eval\n",
    "\n",
    "def find_optimal_combination(candidates, pred_wide, target, metric_fn):\n",
    "    def loss_function(weights):\n",
    "        fc = np.sum(pred_wide[candidates].values * np.array(weights), axis=1)\n",
    "        return metric_fn(pred_wide[target].values, fc)\n",
    "\n",
    "    opt_weights = optimize.minimize(\n",
    "        loss_function,\n",
    "        x0=[1 / len(candidates)] * len(candidates),\n",
    "        constraints=({'type': 'eq', 'fun': lambda w: 1 - sum(w)}),\n",
    "        method='SLSQP',\n",
    "        bounds=[(0.0, 1.0)] * len(candidates),\n",
    "        options={'ftol': 1e-10}\n",
    "    )['x']\n",
    "    \n",
    "    return opt_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for cyclical transformation\n",
    "days_in_month = 31  # Simplified; you might want to adjust this per month/year\n",
    "days_in_week = 7\n",
    "months_in_year = 12\n",
    "\n",
    "# Transform to cyclical features\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['date_day'] / days_in_month)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['date_day'] / days_in_month)\n",
    "\n",
    "df['weekday_sin'] = np.sin(2 * np.pi * df['date_weekday'] / days_in_week)\n",
    "df['weekday_cos'] = np.cos(2 * np.pi * df['date_weekday'] / days_in_week)\n",
    "\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['date_month'] / months_in_year)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['date_month'] / months_in_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df is your time series dataframe and you have a timestamp index\n",
    "# TARGET_COL is the name of the column you want to predict\n",
    "# HORIZON is the number of time steps you want to predict into the future\n",
    "\n",
    "TARGET_COL = 'your_target_column_name'\n",
    "HORIZON = 1  # Change this based on your specific forecasting horizon\n",
    "TRAIN_SIZE = 0.9  # Proportion of data to use for training\n",
    "\n",
    "# Shift the target column to align with the forecast horizon\n",
    "df['shifted_target'] = df[TARGET_COL].shift(-HORIZON)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[TARGET_COL, 'shifted_target'])\n",
    "y = df['shifted_target']\n",
    "\n",
    "# Calculate the index to split the data on\n",
    "split_index = int(len(df) * TRAIN_SIZE)\n",
    "\n",
    "# Split the data into training and testing sets without shuffling\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Check the shape of the datasets\n",
    "print('Training set shape:', X_train.shape, y_train.shape)\n",
    "print('Testing set shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Times Series Forecasting/ Shifted times series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your time series dataset\n",
    "df = pd.read_csv('your_time_series_data.csv')\n",
    "\n",
    "# Specify the target column and forecast horizon\n",
    "TARGET_COL = 'your_target_column'\n",
    "HORIZON = 1  # Example: Predict 1 step ahead\n",
    "\n",
    "# Shift the target column to align with the forecast horizon\n",
    "df['shifted_target'] = df[TARGET_COL].shift(-HORIZON)\n",
    "\n",
    "# Prepare features (X) and target (y), dropping NA values caused by shifting\n",
    "X = df.drop(columns=[TARGET_COL, 'shifted_target']).iloc[:-HORIZON, :]\n",
    "y = df['shifted_target'].dropna()\n",
    "\n",
    "# Initialize time series cross-validator with the desired number of splits\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize your model (using Linear Regression as an example)\n",
    "model = LinearRegression()\n",
    "\n",
    "# List to store the scores for each fold\n",
    "mse_scores = []\n",
    "\n",
    "# Perform time series cross-validation\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the testing data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate and store the mean squared error for this fold\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Output the evaluation results\n",
    "print(\"MSE scores for each fold:\", mse_scores)\n",
    "print(\"Average MSE:\", np.mean(mse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual')\n",
    "plt.plot(y_test.index, y_pred, label='Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Target Variable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto regressive ml approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_forecast(df, window):\n",
    "    d = df.values\n",
    "    x = []\n",
    "    n = len(df)\n",
    "    idx = df.index[:-window]\n",
    "    for start in range(n-window):\n",
    "        end = start + window\n",
    "        x.append(d[start:end])\n",
    "    cols = [f'x_{i}' for i in range(1, window+1)]\n",
    "    x = np.array(x).reshape(n-window, -1)\n",
    "    y = df.iloc[window:].values\n",
    "    df_xs = pd.DataFrame(x, columns=cols, index=idx)\n",
    "    df_y = pd.DataFrame(y.reshape(-1), columns=['y'], index=idx)\n",
    "    return pd.concat([df_xs, df_y], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, test_split=0.15):\n",
    "    n = int(len(df) * test_split)\n",
    "    train, test = df[:-n], df[-n:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multistep Forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_forecast(data, model, steps=10):\n",
    "    forecast = []\n",
    "    for i in range(steps):\n",
    "        one_step_pred = model.predict(np.array(data).reshape(1,-1))[0]\n",
    "        forecast.append(one_step_pred)\n",
    "        _ = data.pop(0)\n",
    "        data.append(one_step_pred)\n",
    "    return np.array(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_two_inputs(x, y, sequence_length, batch_size):\n",
    "    dataset_x = tf.data.Dataset.from_tensor_slices(x)\n",
    "    dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "    dataset = tf.data.Dataset.zip((dataset_x, dataset_y))\n",
    "    dataset = dataset.window(sequence_length+1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(sequence_length+1), y.batch(sequence_length+1))))\n",
    "    dataset = dataset.map(lambda x, y: (x[:-1], y[-1:]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "    def create_dataset(x, y, sequence_length, batch_size):\n",
    "    dataset = tensorflow.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.window(sequence_length+1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda x, y: tensorflow.data.Dataset.zip((x.batch(sequence_length+1), y.batch(sequence_length + 1))))\n",
    "    dataset = dataset.map(lambda x, y: (x[:-1], tensorflow.squeeze(y[-1:], axis = -1)))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tensorflow.data.AUTOTUNE)\n",
    "    return dataset\n",
    "def create_dataset_multiple_outputs(x, y, sequence_length, batch_size, output_steps):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.window(sequence_length + output_steps, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(sequence_length + output_steps), y.batch(sequence_length + output_steps))))\n",
    "    dataset = dataset.map(lambda x, y: (x[:-output_steps], y[-output_steps:]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Attention, AdditiveAttention, Concatenate\n",
    "encoder_input = Input(shape=(None, x_train.shape[1]))\n",
    "encoder_lstm1= Bidirectional(LSTM(64, return_sequences=True))(encoder_input)\n",
    "encoder_lstm2, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(32, return_state=True))(encoder_lstm1)\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [forward_h, forward_c, backward_h, backward_c]\n",
    "\n",
    "decoder_input = Input(shape = (None, x_train.shape[1]))\n",
    "decoder_lstm1 = Bidirectional(LSTM(32, return_sequences=True))(decoder_input, initial_state = encoder_states)\n",
    "decoder_lstm2 = Bidirectional(LSTM(32, return_sequences=True))(decoder_lstm1)\n",
    "\n",
    "attention_layer = Attention()\n",
    "attention_outputs = attention_layer([encoder_lstm2, decoder_lstm2])\n",
    "dense_layer = Dense(1)\n",
    "output_layer = dense_layer(attention_outputs)\n",
    "\n",
    "encoder_decoder_model = Model(inputs = [encoder_input, decoder_input], outputs = [output_layer])\n",
    "encoder_decoder_model.compile(optimizer = 'adam', loss = 'mse', metrics = [RootMeanSquaredError(), MeanAbsoluteError()], run_eagerly=True)\n",
    "encoder_decoder_model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
